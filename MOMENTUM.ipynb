{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "3H9pJQnuuP5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.rand(30, 10)\n",
        "\n",
        "Y = np.random.randint(0, 2, size=(30, 1))\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpZPg6BzuV8U",
        "outputId": "2d30f4c6-1814-4611-f9e3-a3cc725b4f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 10)\n",
            "(30, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W=np.reshape(W,(10,-1))\n",
        "print(W.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4alGddJv2RM",
        "outputId": "8fa6f06d-9868-44f0-f0fd-d8ca78c0bfe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b= np.random.rand(30,1)\n",
        "print(b.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBefLrHNwamv",
        "outputId": "a17d7104-a2b6-4a5b-f89d-b98a166bf59c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = 0  # Bias term\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.01  # Learning rate\n",
        "beta = 0.9    # Momentum term\n",
        "num_iterations = 1000  # Number of iterations\n",
        "\n",
        "# Momentum variables initialization\n",
        "Vdw = np.zeros_like(W)  # Momentum for weights\n",
        "Vdb = 0  # Momentum for bias\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Loss function (binary cross-entropy)\n",
        "def compute_loss(Y, Y_hat):\n",
        "    m = Y.shape[0]\n",
        "    loss = -(1 / m) * np.sum(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat))\n",
        "    return loss\n",
        "\n",
        "# Gradient computation\n",
        "def compute_gradients(X, Y, W, b):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Forward propagation\n",
        "    Z = np.dot(X, W) + b\n",
        "    Y_hat = sigmoid(Z)\n",
        "\n",
        "    # Gradients (backpropagation)\n",
        "    dZ = Y_hat - Y\n",
        "    dw = (1 / m) * np.dot(X.T, dZ)\n",
        "    db = (1 / m) * np.sum(dZ)\n",
        "\n",
        "    return dw, db, Y_hat\n",
        "\n",
        "# Momentum-based gradient descent\n",
        "def momentum_update(W, b, dw, db, Vdw, Vdb, beta, alpha):\n",
        "    # Update momentum terms\n",
        "    Vdw = beta * Vdw + (1 - beta) * dw\n",
        "    Vdb = beta * Vdb + (1 - beta) * db\n",
        "\n",
        "    # Update parameters\n",
        "    W = W - alpha * Vdw\n",
        "    b = b - alpha * Vdb\n",
        "\n",
        "    return W, b, Vdw, Vdb\n",
        "\n",
        "# Training loop\n",
        "for t in range(num_iterations):\n",
        "    # Compute gradients\n",
        "    dw, db, Y_hat = compute_gradients(X, Y, W, b)\n",
        "\n",
        "    # Update weights and biases using momentum\n",
        "    W, b, Vdw, Vdb = momentum_update(W, b, dw, db, Vdw, Vdb, beta, alpha)\n",
        "\n",
        "    # Optionally, print the loss every 100 iterations to monitor progress\n",
        "    if t % 100 == 0:\n",
        "        loss = compute_loss(Y, Y_hat)\n",
        "        print(f\"Iteration {t}: Loss = {loss}\")\n",
        "\n",
        "# Final weights after training\n",
        "print(\"Final weights:\\n\", W)\n",
        "print(\"Final bias:\\n\", b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vOj6TF_vxJz",
        "outputId": "1b8c256b-fda5-4db7-bead-adc2c1c7fd27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Loss = 1.2685125536373538\n",
            "Iteration 100: Loss = 0.8053579043972455\n",
            "Iteration 200: Loss = 0.6930834019309884\n",
            "Iteration 300: Loss = 0.6742322567304272\n",
            "Iteration 400: Loss = 0.6686200633703632\n",
            "Iteration 500: Loss = 0.665004141709815\n",
            "Iteration 600: Loss = 0.6617918194151835\n",
            "Iteration 700: Loss = 0.6587431918662844\n",
            "Iteration 800: Loss = 0.6558176070911415\n",
            "Iteration 900: Loss = 0.6530046374217584\n",
            "Final weights:\n",
            " [[ 0.24540769]\n",
            " [ 0.11892146]\n",
            " [-0.47727657]\n",
            " [-0.23449933]\n",
            " [-0.21206447]\n",
            " [ 0.67516919]\n",
            " [-0.27328403]\n",
            " [-0.03930076]\n",
            " [ 0.33661164]\n",
            " [ 0.00341754]]\n",
            "Final bias:\n",
            " -0.4768093911705387\n"
          ]
        }
      ]
    }
  ]
}